using Flux, Zygote, Functors

# layer with unique connection for each input and output
struct OneToOne
    weight::AbstractArray   # weight matrix
    bias::AbstractArray  # bias vector
    Ïƒ::Function # activation fn
end
@functor OneToOne (weight,bias)

function OneToOne(d::Integer, Ïƒ = identity)
    #weight = randn(Float32, d)
    weight = ones(Float32, d)
    bias = zeros(Float32, d)
    return OneToOne(weight, bias, Ïƒ)
end

function (l::OneToOne)(x)
    return l.Ïƒ(l.weight .* x .+ l.bias)
end

function Zygote.pullback(l::OneToOne, x::AbstractArray)
    y = l.weight .* x .+ l.bias
    z = l.Ïƒ(y)
    
    function B(Î”)
        Î” = Î” .* gradient(layer.Ïƒ, y)[1]  # chain rule applied
        âˆ‡_w = sum(Î” .* x, dims=2)[:]  # Gradient w.r.t. weight
        âˆ‡_b = sum(Î”, dims=2)[:]       # Gradient w.r.t. bias
        return (weight=âˆ‡_w, bias=âˆ‡_b), nothing
    end
    
    return z, B
end

mutable struct Softmax
    weight::AbstractArray
    bias::AbstractArray
    Ïƒ::Function
end
@functor Softmax (weight,bias)

function Softmax(d::Integer)
    weight = randn(Float32,d)
    bias = randn(Float32,d)
    return Softmax(weight,bias)
end

function (Î¸::Softmax)(x)
    return Î¸.Ïƒ(Î¸.weight .* x .+ Î¸.bias)
end

#function Zygote.pullback(Î¸::Softmax,x::AbstractArray)
#    y = Î¸(x)
#    function back(Î”)
#        Î” = Î” .* gradient(Î¸,y)[1]
#        âˆ‡_w = sum(Î” .* x,dims=2)

mutable struct SoftNN
    Ïƒ::Function
end
#
#function (Î´::SoftNN)(x)
#    m,n = size(x)
#    D = euclidean(x)
#    D = D .* (1 .- I(n))

struct DEWAK
    Î¸_e::Dense
    Ï‰::OneToOne
    Îº::Parallel
    Î¸_d::Dense
end
@functor DEWAK

function (m::DEWAK)(X::AbstractMatrix)
    E = (m.Ï‰ âˆ˜ m.Î¸_e)(X)
    D = 1 ./ (euclidean(E) .+ eps(Float32))
    D = wak(D)
    XÌ‚ = m.Î¸_d((D * E')')
    return XÌ‚
end

struct DeePWAK
    Î¸
    Ï‰#::OneToOne
    Ï•
    #Ï…::OneToOne
    Ïˆ
end
@functor DeePWAK

function embedding(m::DeePWAK,X::AbstractMatrix)
    return (m.Ï‰ âˆ˜ m.Î¸)(X)
end

function dist(m::DeePWAK,X::AbstractMatrix)
    E = embedding(m,X)
    return 1 ./ (euclidean(E) .+ eps(Float32))
end

function clust(m::DeePWAK,X::AbstractMatrix)
    E = embedding(m,X)
    C = (softmax âˆ˜ m.Ï•)(E)
    return C' * C
end

function g(m::DeePWAK,X::AbstractMatrix)
    D = dist(m,X)
    P = clust(m,X)
    return wak(D .* P)
end

function(m::DeePWAK)(X::AbstractMatrix)
    E = embedding(m,X)
    G = g(m,X)
    EÌ‚ = (G * E')'
    XÌ‚ = m.Ïˆ(EÌ‚)
    return XÌ‚
end

function H(m::DeePWAK)
    H_Ï‰ = (mean âˆ˜ H)(m.Ï‰.weight)
    return H_Ï‰
end

function softmod(m::DeePWAK,X::AbstractMatrix,Î³)
    D = dist(m,X)
    P = clust(m,X)
    return softmod(D,P,Î³)
end

function loss(m::DeePWAK,X::AbstractArray,Î±,Î²,Î³)
    L = mse(m,X)
    H_m = H(m)
    calH = softmod(m,X,Î³)
    return L + Î± * H_m - Î² * calH
end

function mse(m::Union{Chain,DeePWAK},X::AbstractMatrix)
    return Flux.mse(X,m(X))
end
    
function update!(m::Union{Chain,DeePWAK},loss::Function,opt)
    state = Flux.setup(opt,m)
    l,âˆ‡ = Flux.withgradient(loss,m)
    Flux.update!(state,m,âˆ‡[1])
    return l
end

function train!(m::DeePWAK,X::AbstractMatrix,opt,Î³)
    l = update!(m,Î¸->mse(Î¸,X),opt)
    entropy = update!(m,H,opt)
    modularity = update!(m,Î¸->1/softmod(Î¸,X,Î³),opt)
    return l, entropy, modularity
end

function trainencoder!(m::DeePWAK,X::AbstractMatrix,opt)
    
    

function encoderlayers(m::Integer,d::Integer,l::Integer,Ïƒ=relu)
    #dims = size(X)
    #m = prod(dims[1:(length(dims)-1)])
    s = div(d-m,l)
    ğ = collect(m:s:d)
    ğ[l+1] = d
    #ğ = vcat(ğ,reverse(ğ[1:length(ğ)-1]))
    Î¸ = foldl(ğ[3:length(ğ)],
              init=Chain(Dense(ğ[1] => ğ[2],Ïƒ))) do layers,d
        d_0 = size(layers[length(layers)].weight)[1]
        return Chain(layers...,Dense(d_0 => d,Ïƒ))
    end
end
    
