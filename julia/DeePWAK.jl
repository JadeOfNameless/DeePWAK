using Pkg
Pkg.activate("leiden")
using Flux, CUDA, Functors, ProgressMeter

macro train!(Model)
    quote
        function train!(Î´::$Model)
            @showprogress for _ in 1:Î´.epochs
                for l in Î´.loader
                    x,y = l
                    state = Flux.setup(Î´.opt, Î±);
                    âˆ‡ = Flux.gradient(Î´) do m
                        Î´.loss(m(x),y)
                    end
                    Flux.update!(state,Î´,âˆ‡[1])
                end
            end
        end
    end
end

mutable struct ModelFamily
    params
    model
end
@functor ModelFamily (model,)

mutable struct ModelSpace
    init::Function # âŠ¤ â†’ ModelFamily
    update!::Function # ModelFamily â†’ ModelFamily
end

mutable struct DeePWAK
    Enc::ModelSpace
    Dist::ModelSpace
    Part::ModelSpace
    enc::Autoencoder
    dist
    part
end
@functor DeePWAK (enc,dist,part)

function deepwak(A::ModelSpace,Î”::ModelSpace,Î ::ModelSpace)
    return DeePWAK(A,Î”,Î ,A.init(),Î”.init(),Î .init())
end

function update!(Îº::DeePWAK)
    Îº.Enc.update!(Îº.enc)
    Îº.Dist.update!(Îº.dist)
    Îº.Part.update!(Îº.part)
end

mutable struct Autoencoder
    encoder
    decoder
    loader::Flux.DataLoader
    opt::Flux.Optimiser
    loss::Function
    epochs::Integer
end
@functor Autoencoder (encoder,decoder)
@train! Autoencoder

function (Î±::Autoencoder)(X)
    return Chain(Î±.encoder,Î±.decoder)(X)
end


function encoderlayers(m::Integer,d::Integer,l::Integer,Ïƒ=relu)
    #dims = size(X)
    #m = prod(dims[1:(length(dims)-1)])
    s = div(d-m,l)
    ð = m:s:d
    #ð = vcat(ð,reverse(ð[1:length(ð)-1]))
    Î¸ = foldl(ð[3:length(ð)],
              init=Chain(Dense(ð[1] => ð[2],Ïƒ))) do layers,d
        d_0 = size(layers[length(layers)].weight)[1]
        return Chain(layers...,Dense(d_0 => d,Ïƒ))
    end
end
    
function train!(Î±::Autoencoder)
    @showprogress for _ in 1:Î±.epochs
        for l in Î±.loader
            x,y = l
            state = Flux.setup(Î±.opt, Î±);
            âˆ‡ = Flux.gradient(Î±) do m
                Î±.loss(m(x),y)
            end
            Flux.update!(state,Î±,âˆ‡[1])
        end
    end
end

function autoencoder(X,d,l;epochs=100,Ïƒ=relu,loss=Flux.mse,Î·=0.01,Î»=0,batchsize=1024)
    m,_ = size(X)
    encoder = encoderlayers(m,d,l)
    decoder = encoderlayers(d,m,l)
    loader = Flux.DataLoader((X,X),batchsize=batchsize,shuffle=true) |> gpu
    opt = Flux.Optimiser(Flux.AdamW(Î·),Flux.WeightDecay(Î»))
    Î± = Autoencoder(encoder,decoder,loader,opt,loss,epochs) |> gpu

    train!(Î±)
    return Î±
end
    
mutable struct DistEnc
    encoder
    loader
    opt
    loss
    epochs
end
@functor DistEnc (encoder,)
@train! DistEnc

function (Î´::DistEnc)(E)
    m,n = size(E)
    pairs = genpairs(E)
    D = Î´.encoder(pairs)
    D = reshape(D,(n,n)) .* (1 .- (I(n)|>gpu))
    D = D ./ (sum(D,dims=1) .+ eps(eltype(D)))
    return D
end

function train!(Î±::Autoencoder,Î´::DistEnc)
    loss(m) = function(m)
    map(1:Î´.epochs) do _
        map(Î±.loader) do (x,y)
            E = Î±.encoder(x)

function distenc(X::AbstractMatrix,Î±::Autoencoder,l::Integer;
                 epochs=100,Ïƒ=relu,loss=Flux.mse,Î·=0.01,Î»=0,batchsize=1024)
    m,n = size(X)
    E = Î±.encoder(X)
    d,n = size(E)
    encoder = encoderlayers(d*2,1,l)
    loader = Flux.DataLoader((X,X),batchsize=batchsize,shuffle=true) |> gpu
    opt = Flux.Optimiser(Flux.AdamW(Î·),Flux.WeightDecay(Î»))
    Î´ = DistEnc(encoder,loader,opt,loss,epochs) |> gpu

    #train!(Î±,Î´)
    return Î´
end
    
function genpairs(X::CuArray)
    m, n = size(X)
    X_1 = repeat(X, 1, n)
    X_2 = reshape(repeat(X, n), m, n^2)
    return vcat(X_1, X_2)
end

function L_Î´(Î±::Autoencoder,X,Y,D)
    m,n = size(Y)
    E = Î±.encoder(X)
    D = reshape(D,(n,n)) .* (1 .- (I(n)|>gpu))
    D = D ./ (sum(D,dims=1) .+ eps(eltype(D)))
    YÌ‚ = Î±.decoder(E * D)
    Flux.mse(YÌ‚,Y)
end
