using Pkg
Pkg.activate("leiden")

using CSV, DataFrames, Tensors, CategoricalArrays, Distributions,
    Flux, LinearAlgebra, SparseArrays, Distances, ProgressMeter, CUDA
using Leiden
using ThreadTools
using JLD2

function zcat(args...)
    cat(args...,dims=3)
end

function mapmap(f,args...)
    map((args...)->map(f,args...),args...)
end

function tmapmap(f,args...)
    tmap((args...)->map(f,args...),args...)
end

function tmapreduce(f,rf,args...)
    rf(tmap(f,args...)...)
end

function scaledat(X::AbstractArray,dims=1)
    Y = X ./ maximum(abs.(X),dims=dims)
    Y[isnan.(Y)] .= 0
    return Y
end

function sampledat(X::AbstractArray,k)
    _,n = size(X)
    sel = sample(1:n,k,replace=false)
    test = X[:,sel]
    train = X[:,Not(sel)]
    return test,train
end

function wak(G)
    #Matrix -> Matrix
    #returns G with row sums normalized to 1
    W = sum(G,dims=2)
    K = G ./ W
    K[isnan.(K)] .= 0
    return K
end

function ehat(E,D,G)
    (wak(G .* D) * E')'
end

function ð•ƒ(X,Î¸,E,D,G)
    Flux.mse(X,(Î¸ âˆ˜ ehat)(E,D,G))
end

function partitionmat(C)
    (sum âˆ˜ map)(1:maximum(C)) do c
        x = C .== c
        return x * x'
    end
end

function diffuse(X,Î¸,E,D,G,P,d)
    M = P .* G
    M = wak(M .* D)
    foldl(1:d,init=(M,[])) do (Mâº,L),_
        Mâº = Mâº * M
        Lâº = Flux.mse(X,Î¸((Mâº * E')'))
        L = vcat(L,Lâº)
        return Mâº,L
    end
end

frac = 10
b = 32
bfÅ‹ = 1:32
ð¤ = 1:128
ð›„ = rand(Uniform(0.1,3),128);
ð = 128

dat = (DataFrame âˆ˜ CSV.File)("z_dat.csv",normalizenames=true);
dat = (scaledat âˆ˜ Matrix)(dat[:,2:end]);
dat = hcat(filter(x->sum(x) != 0,eachslice(dat,dims=2))...);

n,m = size(dat)
n_Y = Integer(2^round(log2(n) - log2(frac)))
n_X = n - n_Y

tmp = map(_->sampledat(dat',n_Y),1:b);

test = map(x->x[1],tmp);
train = map(x->x[2],tmp);

X = train |> gpu;
Y = test |> gpu;

#layers = map(Å‹->Chain(Dense(m => Å‹,relu),Dense(Å‹ => m)),bfÅ‹);
ðš¯ = map(1:b) do _
    l = map(Å‹->Chain(Dense(m => Å‹,relu),Dense(Å‹ => m)),bfÅ‹)
    return Parallel(vcat,l...) |> gpu
end;

loader = map(X) do X_b
    Flux.DataLoader((X_b,repeat(X_b,outer=(length(bfÅ‹),1))), batchsize=1024, shuffle=true)
end;

opt = Flux.Optimiser(Flux.AdamW(0.01), Flux.WeightDecay(0))

@showprogress for _ in 1:100
    map(ðš¯,loader) do Î˜,l
        loss = (X,Y)->Flux.mse(Î˜(X),Y)
        Flux.train!(loss,Flux.params(Î˜),l,opt)
    end
end

ðš¯_s = map(Flux.state,ðš¯);
jldsave("bfTheta.jld2";ðš¯_s)
ðš¯_s = JLD2.load("bfTheta.jld2", "ðš¯_s");
Flux.loadmodel!(ðš¯,ðš¯_s)

function embedding(Î˜,X)
    map(Î˜.layers) do Î¸
        Î¸[1](X)|>cpu
    end
end

function distmat(E)
    1 ./ (pairwise(Euclidean(),E,E) + (I*Inf));
end

function perm(D,n)
    K = sortperm(D,dims=1,rev=true) .% n
    K[K .== 0] .= n
    return K
end

function adjmat(K,ð¤,n)
    G = map(1:maximum(ð¤)) do k
        sparse(1:n,K[:,k],1,n,n)
    end
    G = map(ð¤) do k
        foldl(+,G[1:k])
    end
    return G
end
        
function ð•ƒ_Å‹k(X,Î¸,E,D,ð†)
    E = E |> gpu
    D = D |> gpu
    map(ð†) do G
        G = G |> gpu
        return ð•ƒ(X,Î¸[2],E,D,G)
    end
end

ð„_X = map(embedding,ðš¯,X);
ð„_Y = map(embedding,ðš¯,Y);

trees = mapmap(KDTree,ð„_X);

knn_Y = tmapmap(trees,ð„_Y) do tree,E
    map(k->knn(tree,E,k),ð¤)
end;


ðƒ = mapmap(distmat,ð„_X)

ðƒ_X= tmap(E->1 ./ (pairwise(Euclidean(),E,E) + (I*Inf)),ð„);
ðŠ_X = tmap(ðƒ) do D
    K = sortperm(D,dims=1,rev=true) .% n
    K[K .== 0] .= n
    return K
end;

ðŠ_decomp = tmap(ðŠ) do K
    map(1:maximum(ð¤)) do k
        sparse(1:n,K[:,k],1,n,n)
    end
end;

ð† = tmap(ðŠ_decomp) do K
    map(ð¤) do k
        foldl(+,K[1:k])
    end
end;

#@showprogress ð† = map(ðŠ) do K
#    map(ð¤) do k
#        mapslices(x->sparsevec(x[1:k],1,n),K,dims=2)
#    end
#end
#@save "knns.bson" ð†
#
#ð† = map(ðŠ, ðƒ) do K,D
#    map(ð¤) do k
#        G = mapslices(x->(wak âˆ˜ sparsevec)(x[1:k,1],x[1:k,2],n),
#                      cat(K,D,dims=3),dims=(2,3))
#        return dropdims(G,dims=3)
#    end
#end

L_Å‹k = mapreduce(hcat,Î˜.layers,ð„,ðƒ,ð†) do Î¸,E,D,Gs
    E = E |> gpu
    D = D |> gpu
    map(Gs) do G
        G = G|>gpu
        return ð•ƒ(X',Î¸[2],E,D,G)
    end
end

Tables.table(L_Å‹k) |> CSV.write("MSEagma_k.csv")
k,Å‹ = argmin(L_Å‹k).I 

Î¸ = Î˜.layers[Å‹]
E = ð„[Å‹] |> gpu
D = ðƒ[Å‹]
G = ð†[Å‹][k]

ð‚ = tmap(Î³->Leiden.leiden(wak(D .* G),"mod++", Î³ = Î³),ð›„)
(Tables.table âˆ˜ hcat)(ð‚...) |> CSV.write("clusters.csv")

ð = map(partitionmat,ð‚)

D = D |> gpu

Láµ§ = map(ð) do P
    P = P |> gpu
    return ð•ƒ(X',Î¸[2],E,D,P)
end
argmin(Láµ§)

DataFrame(gamma=ð›„,MSE=Láµ§) |> CSV.write("MSEgamma.csv")

G = G |> gpu
L_Î³d = mapreduce(hcat,ð) do P
    P = P |> gpu
    _,L = diffuse(X',Î¸[2],E,D,G,P,ð)
    return L
end

Tables.table(L_Î³d) |> CSV.write("MSEgamma_d.csv")

DataFrame(gamma=ð›„,MSE=Láµ§G) |> CSV.write("clustergraphloss.csv")

Láµ§GÂ² = map(ð) do P
    P = P |> gpu
    M = P .* G
    return Flux.mse(X',(Î¸_Å‹[2] âˆ˜ ehat)(E,D,M * M))
end

tmp = G .* P
tmp=wak(tmp .* D)
diffuse(tmp,10)

L_diff = ((x->hcat(x...)) âˆ˜ map)(ð) do P
    P = P |> gpu
    M = P .* G
    M = wak(M .* D)
    _,L = diffuse(M,50)
    return L
end

(Tables.table âˆ˜ vcat)(ð›„,L_diff) |> CSV.write("clustdiffloss.csv")
