\section{Introduction}
At its heart, clustering is an optimization problem of how to partition a data set given some loss function.

Though applications of deep learning to classification is well established, self-supervised classification has been much less thoroghly explored.
Deep clustering is an active area of research\cite{ren2022deep}.
Similarly to DeepCluE\cite{huang2023deepclue}, we use an ensemble clustering method.
But rather than creating a single consensus partition, we aim to maximise independence between submodels.
%A common pretraining method is contrastive clustering\cite{li2020contrastive}.
%This is a method of self-supervised feature detection consisting of generating synthetic pairs of data by applying various image transformations.

Our approach uses what we believe to be a previously unexplored combination of an information bottleneck with self-supervised denoising.
Like ClAM\cite{saha2023endtoend} it is end-to-end differentiable.
Unlike ClAM,

\subsection{Notation}
Capital letters indicate matrices. Subscripts indicate indices. Superscripts indicate dimensionality.
A circumflex indicates a reconstruction of data by a predictor. Lowercase Greek letters indicate tunable parameters.
Capital Greek letters indicate lists of parameters for multiple models.
Boldface Greek letters indicate parameter spaces.
For parameters $\theta$, $\theta^{m \to d}$ indicates parameters for a model that accepts an input of dimension $m$ and returns an output of dimension $d$.
$\fn^{n \to m}$ indicates a layer with input dimension $n$, output dimension $m$, and activation function $\fn$.

$\odot$ is the Hadamard product.

$\map^n_{i=k}\expr$ indicates mapping $\expr$ over $i \in \{k:n\}$.

$\map^{X,Y}_{x,y}\expr$ indicates mapping $\expr$ over $(x \in X,y \in Y)$.

$\List(\Type,n)$ indicates a list of $n$ elements of \Type.

We will sometimes find it necessary to distinguish between a model, a model's architecture, and a model's parameters.
$f:\Model(n,m)$ indicates a model $f:\mathbb{R}^n \to \mathbb{R}^m$.

$\mathcal{F}:\Arch(n,m)$ indicates an architecture for a model $\Model(n,m)$.

$\theta:\Params(\mathcal{F})$ indicates the parameters for $\mathcal{F}$.

We use the notation $\mathcal{F}(\theta)$ to indicate a model architecture $\mathcal{F}$ parameterized by $\theta$.
$\mathcal{F}(\theta)(X)$ indicates passing data $X$ to the model $\mathcal{F}(\theta)$.
We write this as a curried function to emphasize that $\mathcal{F}(\theta)$ is stateful.


\subsection{noise2self}

Batson \& Royer\cite{batson2019noise2self} identify a class of denoising functions which can be optimised using only unlabeled noisy data.

Let $J \in \mathcal{J}$ be independent partitions of noisy data $X$. Let $\mathcal{F}(\theta)$ be a family of predictors of $X_J$ with tunable parameters $\theta \in \mathbf{\Theta}$ that depends on its complement $X_{J^C}$

\begin{equation}
  \hat{X}_J=\mathcal{F}(\theta)(X_{J^C})
\end{equation}

In other words, $\mathcal{F}$ predicts each data point $X_J$ from some subset of the data excluding $X_J$. 

  The optimal $\theta$ is given by

\begin{equation}
  \ntos_\theta^{\mathbf{\Theta}}[\mathcal{F}(\theta),X] := \argmin_\theta^{\mathbf{\Theta}}[\sum_{J}^{\mathcal{J}}\mathbb{E}[[X_J-\mathcal{F}(\theta)(X_{J^C})]]^2]
\end{equation}



\subsection{Graph diffusion}

Our choice of $\mathcal{F}$ is adapted from DEWAKSS\cite{tjarnberg2021}. The parameters we want to tune generate a graph $G$ from embeddings $E$. The adjacency matrix of any graph can be treated as a transition matrix (or weighted affinity kernel) by setting the diagonal to 0 and normalizing columns to sum to 1. We call this the \WAK function.

\input{algorithms/wak.tex}

For each embedding, an estimate is calculated based on its neighbors in the graph. This can be expressed as matrix multiplication.

\begin{equation}
\hat{E} := \mathrm{WAK}(G)E^\top
\end{equation}

Though DEWAKSS uses a $k$-NN graph, any adjacency matrix will do.
A clustering can be expressed as a graph where points within a cluster are completely connected and clusters are disconnected.

Let $C^{c \times n}$ be a matrix representing a clustering of $n$ points into $c$ clusters. Let each column be a 1-hot encoding of a cluster assignment for each point. We can obtain a partition matrix $P^{n \times n}$ by

\begin{equation}
  P := C^\top C
\end{equation}

Informally, this can be equated to position-independent attention with data points as tokens and the batch size as the context window.

