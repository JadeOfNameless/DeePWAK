\section{Introduction}
  %Clustering is a special case of sparse dictionary learning where all features are discrete.
Fundamentally, unsupervised learning is a problem of finding the latent space of a data set.
Denoising, compression, and clustering all ultimately attempt to solve this problem, but approach it from different directions.
Denoising aims to remove spurious dimensions whereas compression looks to select informative dimensions.
The connection to clustering is less obvious, but no less foundational\cite{9830658}.

Compression is a generalization of supervised learning. 
We propose that 

At its heart, clustering is an optimization problem of how to partition a data set given some loss function.

There are a few ways to conceptualize this isomorphism.

\paragraph{Clustering as principal components in latent space}
Data are sparse in latent space.
We could think of a cluster as a vector along which data are relatively dense.
Though it might na\"ively be expected that enforcing orthogonality would be a problem,  
it's important to keep in mind that latent space is \textit{really high dimensional}.
In practice most vectors can be treated as ``almost orthogonal''.

\paragraph{Clustering as (lossy) compression}
A cluster can be thought of as an injection of a subset of the data onto some representative value/vector.
This is essentially a decomposition of the data into a 1-hot incidence matrix of cluster assignments and a matrix of central cases.
As an illustration, consider data $X \in \mathbb{R}^{m \times n}$ where $n$ is the number of data points and $m$ is the dimensionality.
We cluster $X$ into $k$ clusters.
We obtain a matrix of central cases for each cluster $C \in \mathbb{R}^{k \times m}$.
The incidence matrix is given by $K \in \mathbb{B}^{n \times k}$.
We now have a lossy approximation of $X$ with the matrix decomposition

\begin{equation}
  X \approx KC
\end{equation}

\paragraph{Clustering as denoising}
Compression loss isn't always a problem.
Identifying informative latent features means throwing out uninformative latent features.

\paragraph{Clustering as sparse dictionary learning}
Optimizing (1) is essentally the goal of sparse dictionary learning, where
$C$ is the dictionary and $K$ is the key.


\paragraph{Clusters as attractor states}
One of the most beautiful results from information theory is that \textit{prediction is thermodynamic work}.
The entropy of any dataset can be expressed in terms of transition probabilities.
The entropy of a partition can be expressed as a mask applied to this transition matrix.
An optimal clustering has maximal entropy\cite{e17010151}.
This is equivalent to saying that the optimal partition is the one that \textit{deletes} the most information from the data when adopted as a prior.

\paragraph{Clustering as the optimal way of labeling a data set}
Though clustering is normally phrased as an unsupervised learning problem,
it may be informative to frame it as \textit{the dual of supervised learning}.
In other words, instead of finding the features that contain the most information given a labeling,


Code is available at \url{https://github.com/kewiechecki/DeePWAK}.

\section{Background}

\subsection{The unreasonable effectiveness of transformers}

\subsection{The unreasonable effectiveness of MoE}

\subsection{Notation}
Details are in Appendix \ref{app:notation}.

%Though applications of deep learning to classification is well established, self-supervised classification has been much less thoroghly explored.
%Deep clustering is an active area of research\cite{ren2022deep}.
%Similarly to DeepCluE\cite{huang2023deepclue}, we use an ensemble clustering method.
%But rather than creating a single consensus partition, we aim to maximise independence between submodels.
%A common pretraining method is contrastive clustering\cite{li2020contrastive}.
%This is a method of self-supervised feature detection consisting of generating synthetic pairs of data by applying various image transformations.

%Our approach uses what we believe to be a previously unexplored combination of an information bottleneck with self-supervised denoising.
%Like ClAM\cite{saha2023endtoend} it is end-to-end differentiable.
%Unlike ClAM,

\section{Related Work}

\subsection{Sparse autoencoders}
Sparse dictionary learning has become a powerful tool for interpretation of neural networks. 
The activations of a neural network can be decomposed into a linear combination of dictionary features\cite{cunningham2023sparse}.
There are several limitations to this technique as currently applied.
The number of features must be preselected.
Interpretation of features still relies on manual inspection,
but there is no structure or organization imposed on the dictionary.
This results in a tradeoff between feature interpretability and feature search space. 
It would be desirable to be able to selectively decompose higher level features into lower level features.
One of the most interesting properties of SAEs is that features seem to be organized into clusters\cite{bricken2023monosemanticity}.
This suggests the possibility of merging low level features or splitting high level features.



\subsection{Denoising the data explains the data}
For a large class of denoising functions, it is possible to find optimal parameters using only unlabeled noisy data\cite{batson2019noise2self}.

See Appendix \ref{app:ntos}.

\subsection{Diffusion with weighted affinity kernels}

$\ntos$ is particularly useful for finding optimal parameters for generating a graph\cite{tjarnberg2021}.
(see Appendix \ref{app:DEWAKSS})
The adjacency matrix $G$ of any graph can be treated as a transition matrix (or weighted affinity kernel) by setting the diagonal to 0 and normalizing columns to sum to 1. We call this the \WAK function (Algorithm \ref{alg:WAK}).

For each value in data $X$, an estimate is calculated based on its neighbors in the graph. This can be expressed as matrix multiplication.

\begin{equation}
  \label{eq:WAK}
\hat{X} := \WAK(G)X^\top
\end{equation}


\subsection{Partitioned weighted affinity kernels} 

Though DEWAKSS uses a $k$-NN graph, any adjacency matrix will do.
A clustering can be expressed as a graph where points within a cluster are completely connected and clusters are disconnected.

Let $K^{k \times n}$ be a matrix representing a clustering of $n$ points into $k$ clusters. Let each column be a 1-hot encoding of a cluster assignment for each point. We can obtain a partition matrix $P \in \mathbb{R}^{n \times n}$ by what we'll call the \textit{partitioned weighted affinity kernel} (\PWAK) function.

\begin{equation}
  \label{eq:PWAK}
  \PWAK(K) := \WAK(K^\top K)
\end{equation}

This lets us define a loss function

\begin{equation}
  \mathcal{L}_{\mathPWAK}(K,X) := \mathbb{E}[\mathPWAK(K)X^\top - X]^2
\end{equation}


\PWAK can be extended to soft cluster assignment, making it possible to learn $K$ via SGD.
We will refer to a model of this sort as a \Partitioner to emphasize that while it returns logits corresponding to classifications, there are no labels on the training data.
See Appendix \ref{app:Partitioner} for details.

We can now train a model to classify unlabeled data into an undefined number of clusters with no prior distribution in $\mathcal{O}(n^2)$ time!


\subsection{Diffusion in embedding space}

A common problem in kernel diffusion is nonlinearity.
We have no reason to expect features of the data are linearly separable.
We can however use an autoencoder to find a linearly separable embedding \cite{xie2016unsupervised}.
Since we have no reason to expect a priori that a given embedding will be linearly separable,
this generally involves applying a linear transformation to the embeddings and letting SGD figure it out.

We refer to this combination of an autoencoder with a \Partitioner trained on a \PWAK loss function as \textit{denoising by deep learning of a partitioned weighted affinity kernel} (\DeePWAK).

For an encoder $\theta$ and a decoder $\phi$, our loss function is now

\begin{equation}
  \mathcal{L}_{\mathDeePWAK}(\theta,\phi,K,X) := \mathbb{E}[\phi(\mathPWAK(K)\theta(X)^\top) - X]^2
\end{equation}

This is very similar to the SAE loss function, as both aim to impose linear separability in an embedding space.
There is one subtle distinction that has wide-reaching ramifications.
$\mathcal{L}_{\DeePWAK}$ is calculated for an entire minibatch rather than for each input value.
Rather than directly learning features, \DeePWAK attempts to recognize values that can be used to impute each other.
This results in not only sparse features, but sparse clusters.
