\documentclass{article}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{url}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}

\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage[backend=biber]{biblatex}
\addbibresource{refs.bib}

\graphicspath{ {./fig/} }

\usepackage{tikz}
\usetikzlibrary{arrows.meta, bending, positioning}

\newcommand{\txtop}[1]{\mathop{\mathrm{#1}}\limits}
\newcommand{\tanhl}{\txtop{tanh}}
\newcommand{\softmax}{\txtop{softmax}}
\newcommand{\linear}{\txtop{linear}}
\newcommand{\sigmoid}{\txtop{sigmoid}}
\newcommand{\DeePWAK}{\txtop{DeePWAK}}
\newcommand{\encoder}{\txtop{encoder}}
\newcommand{\decoder}{\txtop{decoder}}
\newcommand{\partitioner}{\txtop{partitioner}}

\date{\today}

\title{DeePWAK: Clustering \& Denoising Intertwined}

\author{Keira Wiechecki \\
  Center for Genomics \& Systems Biology \\
  New York University \\
	%\And 
        %Jade Zaslavsky
	%Lionel Christiaen \\
}

\begin{document}
\maketitle

\begin{abstract}
  Clustering is a special case of sparse dictionary learning where all features are discrete.
Here we introduce Denoising by Deep learning of a Partitoned Weighted Affinity Kernel (DeePWAK). 
\end{abstract}

\section{Introduction}
Though the applications of deep learning to classification is well established,

\subsection{noise2self}

Batson \& Royer\cite{batson2019noise2self} identify a class of denoising functions which can be optimised using only unlabeled noisy data.

Let $J \in \mathcal{J}$ be independent partitions of noisy data $X$. Let $\mathcal{F}(\theta)$ be a family of predictors of $X_J$ with tunable parameters $\theta \in \Theta$ that depends on its complement $X_{J^C}$

\begin{equation}
  \hat{X}_J=\mathcal{F}(\theta)(X_J^C)
\end{equation}

In other words, $\mathcal{F}$ predicts each data point $X_J$ from some subset of the data excluding $X_J$. 

  The optimal $\theta$ is given by

\begin{equation}
  \underset{\theta}{\overset{\Theta}{\mathrm{noise2self}}}[\mathcal{F}(\theta),X] := \underset{\theta}{\overset{\Theta}{\mathrm{argmin}}}[\sum_{J}^{\mathcal{J}}\mathbb{E}||X_J-\mathcal{F}(\theta)(X_{J^C})||^2]
\end{equation}



\subsection{Graph diffusion}

Our choice of $\mathcal{F}$ is adapted from DEWAKSS\cite{tjarnberg2021}. The parameters we want to tune generate a graph $G$ from embeddings $E$. The adjacency matrix of any graph can be treated as a transition matrix (or weighted affinity kernel) by setting the diagonal to 0 and normalizing columns to sum to 1. We call this the $\mathrm{WAK}$ function. For each embedding, an estimate is calculated based on its neighbors in the graph. This can be expressed as matrix multiplication.

\begin{equation}
\hat{E} := \mathrm{WAK}(G)E^\top
\end{equation}

Though DEWAKSS uses a $k$-NN graph, any adjacency matrix will do.
A clustering can be expressed as a graph where points within a cluster are completely connected and clusters are disconnected.

Let $C^{c \times n}$ be a matrix representing a clustering of $n$ points into $c$ clusters. Let each column be a 1-hot encoding of a cluster assignment for each point. We can obtain a partition matrix $P^{n \times n}$ by

\begin{equation}
  P := C^\top C
\end{equation}

\section{Notation}
Capital letters indicate matrices. Subscripts indicate indices. Superscripts indicate dimensionality. A circumflex indicates a reconstruction of data by a predictor. Lowercase Greek letters indicate tunable parameters. Capital Greek letters indicate parameter spaces. For parameters $\theta$, $\theta^{m \to d}$ indicates parameters for a model that accepts an input of dimension $m$ and returns an output of dimension $d$.

\section{Architecture}

The DeePWAK constructor has the type signature

\begin{equation}
  \mathrm{DeePWAK} := \forall m,d,c :\mathbb{N} \to (\mathbb{R}^m \to \mathbb{R}^d) \to (\mathbb{R}^m \to \mathbb{R}^c) \to (\mathbb{R}^d \to \mathbb{R}^m) \to \mathbb{R}^m \to \mathbb{R}^m
\end{equation}

It consists of an encoder, partitioner, and decoder.

\begin{algorithm}
  \caption{DeePWAK constructor}\label{alg:cap}
  \begin{algorithmic}[1]
    \State \textbf{data} $\mathrm{DeePWAK}${
    \State $\mathrm{encoder} : \exists m,d : \mathbb{N} \to \mathbb{R}^m \to \mathbb{R}^d$
    \State $\mathrm{partitioner} : \exists m,c : \mathbb{N} \to \mathbb{R}^m \to \mathbb{R}^c$
    \State $\mathrm{decoder} : \exists d,m : \mathbb{N} \to \mathbb{R}^d \to \mathbb{R}^m$
    }
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{DeePWAK application}\label{alg:cap}
  \begin{algorithmic}[1]
    \State \textbf{function} $\mathrm{DeePWAK}(\theta, \pi, \phi)(X : \mathbb{R}^{m \times n})${
    \State $E \gets \theta(X)$
    \State $C \gets (\mathrm{softmax} \circ \pi)(X)$
    \State $P \gets C^\top C$
    \State $G \gets \mathrm{WAK}(P)$
    \State $\hat{E} \gets (GE^\top)^\top$
    \State $\hat{X} \gets \phi(\hat{E})$
    \State \textbf{return} $\hat{X}$
    }
  \end{algorithmic}
\end{algorithm}

\subsection{Multihead DeePWAK}

\section{A concrete example from microscopy data}

\begin{figure}[ht]
     \begin{subfigure}[t]{0.10\textwidth}
        \input{tikz/layers.tex}
         \caption{}
         \label{fig:}
     \end{subfigure}

     \hfill
     \begin{subfigure}[t]{0.10\textwidth}
        \input{tikz/DeePWAK.tex}
         \caption{}
         \label{fig:}
     \end{subfigure}
     
     \vspace{1cm}
     \begin{subfigure}[t]{0.10\textwidth}
        \input{tikz/DeePWAKBlock.tex}
         \caption{}
         \label{fig:}
     \end{subfigure}

     \hfill
     \begin{subfigure}[t]{0.10\textwidth}
       \input{tikz/ontologizer.tex}
       \caption{}
       \label{fig:}
     \end{subfigure}
     
     \caption{(a) Architecture of one DeePWAK head. (b) Training loop for DeePWAK block.}
     \label{fig:}
\end{figure}


\section{Results}

\subsection{Multihead DeePWAK learns sparse representations}

\section{Discussion}

\printbibliography

\end{document}
