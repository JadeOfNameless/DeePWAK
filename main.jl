using Pkg
Pkg.activate("julia/leiden")
#include("julia/DeePWAK.jl")
include("julia/fns.jl")

using Flux, ProgressMeter, CUDA, LinearAlgebra
using Distributions, Distances
using Zygote, Functors
using Zygote: @adjoint
using CSV, DataFrames, MLDatasets

# layer with unique connection for each input and output
mutable struct OneToOne
    weight::AbstractArray   # weight matrix
    bias::AbstractArray  # bias vector
    Ïƒ::Function # activation fn
end
@functor OneToOne (weight,bias)

mutable struct Softmax
    weight::AbstractArray
    bias::AbstractArray
end
@functor Softmax (weight,bias)

# Constructor for initializing the layer with a diagonal matrix
function OneToOne(in::Integer, out::Integer, Ïƒ = identity)
    # initializing with non-zero diagonal values
    weight = Flux.Zeros(out, in) .+ Diagonal(fill(0.01, min(in, out))) 
    bias = zeros(out)
    return OneToOne(weight, bias, Ïƒ)
end

# Forward pass for the custom layer
(layer::OneToOne)(x) = layer.Ïƒ(Diagonal(layer.weight) .* x .+ layer.bias)

function Zygote.pullback(layer::OneToOne,x::AbstractArray)
    y = layer(x)
    function back(Î”)
        # Here you ensure only diagonal elements are considered
        # Adjust this gradient calculation as necessary for your use case
        ddiag = diag(Î” * x')
        return y,ddiag
    end
    
    return y, back
end
    
function loss(X,Î¸)
    W_e = Î¸.layers[1].weight
    H_e = (mean âˆ˜ H)(W_e)

    W_d = Î¸.layers[2].weight
    H_d = (mean âˆ˜ H)(W_d)
    L = Flux.mse(X,Î¸(X))
    return ((H_e + H_d) / 2) + L
end

function H(W;dims=1)
    W = abs.(W) .+ eps(eltype(W))
    W = W ./ sum(W,dims=dims)
    return -sum(W .* log2.(W),dims=dims)
end

function wak(G::AbstractArray)
    m, n = size(G)
    G = G .* (1 .- I(n))
    G = G ./ (sum(G,dims=1) .+ eps(eltype(G)))
    return G
end
function wak(G::CuArray)
    m, n = size(G)
    G = G .* (1 .- (I(n)|>gpu))
    G = G ./ (sum(G,dims=1) .+ eps(eltype(G)))
    return G
end

function euclidean(x::CuArray{Float32})
    x2 = sum(x .^ 2, dims=1)
    D = x2' .+ x2 .- 2 * x' * x
    # Numerical stability: possible small negative numbers due to precision errors
    D = sqrt.(max.(D, 0) .+ eps(Float32))  # Ensure no negative values due to numerical errors
    return D
end


function loss(X,Î¸)
    md,dd,dm = Î¸.layers
    W_dd = dd.weight
    #W_dd = mapreduce(x->x.weight,vcat,dd.layers)

    H_md = (mean âˆ˜ H)(md.weight)
    H_dd = (mean âˆ˜ H)(W_dd)
    #H_E = (H_md + H_dd) / 2
    H_dm = (mean âˆ˜ H)(dm.weight)
    H_E = (H_md + H_dd + H_dm) / 3

    E = Î¸[1:2](X)
    D = 1 ./ (euclidean(E) .+ eps(Float32))
    D = wak(D)

    H_D = (mean âˆ˜ H)(D)
    L = Flux.mse(X,Î¸[3]((D * E')'))
    return ((H_E + H_D) / 2) * L
    #L = Flux.mse(X,Î¸(X))
    #return H_E * L
end



function OneToOne(in::Integer, out::Integer, Ïƒ = identity)
    weight = randn(Float32, out)
    bias = zeros(Float32, out)
    return OneToOne(weight, bias, Ïƒ)
end

function (layer::OneToOne)(x)
    return layer.Ïƒ(layer.weight .* x .+ layer.bias)
end

function Zygote.pullback(layer::OneToOne, x::AbstractArray)
    pre_activation = layer.weight .* x .+ layer.bias
    y = layer.Ïƒ(pre_activation)
    
    function OneToOne_pb(Î”)
        Î” = Î” .* gradient(layer.Ïƒ, pre_activation)[1]  # chain rule applied
        diag_grad = sum(Î” .* x, dims=2)[:]  # Gradient w.r.t. weight
        bias_grad = sum(Î”, dims=2)[:]       # Gradient w.r.t. bias
        return (weight=diag_grad, bias=bias_grad), nothing
    end
    
    return y, OneToOne_pb
end

dat = MNIST.traindata();
X = vcat(eachslice(dat[1],dims=1)...);

#dat = MNIST()
#X = vcat(eachslice(dat.features,dims=1)...);

m,n = size(X)

epochs = 1000
d = 50
Î· = 0.01
Î» = 0.01
batchsize=1024

opt = Flux.Optimiser(Flux.AdamW(Î·),Flux.WeightDecay(Î»))

Î¼ = mean(X,dims=2);
X_0 = X .- Î¼;
Î£ = cov(X_0,dims=2);
Î›,U = eigen(Î£);
W = U * Diagonal(sqrt.(1 ./(Î› .- minimum(Î›) .+ eps(Float32)))) * U';
XÌƒ = W * X |> gpu;

loader = Flux.DataLoader((XÌƒ,XÌƒ),batchsize=batchsize,shuffle=true) |> gpu

Î¸_md = Dense(m => d)
Î¸_dd = OneToOne(d,d)
Î¸_dm = Dense(d => m)

Î¸ = Chain(Î¸_md,Î¸_dd,Î¸_dm) |> gpu

x,y = first(loader)
state = Flux.setup(opt,Î¸);
L,âˆ‡ = Flux.withgradient(x->loss(x,Î¸),x)

# Forward pass
y, back = Zygote.pullback(x->loss(x,Î¸), x)

# Backward pass
Î” = randn(Float32,size(y))|>gpu
grads = back(Î”)

ð‹ = []
@showprogress map(1:epochs) do _
    L = map(loader) do (x,y)
        state = Flux.setup(opt,Î¸)
        L,âˆ‡ = Flux.withgradient(Î¸->loss(x,Î¸),Î¸)
        Flux.update!(state,Î¸,âˆ‡[1])
        return L
    end
    push!(ð‹,L)
end


d = 2
l = 2

X = X |> gpu
Î± = autoencoder(X,d,l;Î»=0.01)

Î´ = distenc(X,Î±,2*d-2;Î»=0.01)

x,y = first(Î´.loader)
E = Î±.encoder(x)

function loss(m)
    Î´.loss(Î±.decoder(E*m(E)),y)
end
loss(Î´)

âˆ‡ = Flux.gradient(loss,Î´)
Flux.update!(state_Î´,Î´,âˆ‡[1]);

state = Flux.setup(Î±.opt, Î±);
L,âˆ‡ = Flux.withgradient(m->Î±.loss(m(x),y),Î±);
Flux.update!(state,Î±,âˆ‡[1]);

@showprogress for _ in 1:Î±.epochs
     map(Î±.loader) do (x,y)
        state = Flux.setup(Î±.opt, Î±);
        âˆ‡ = Flux.gradient(Î±) do m
            Î±.loss(m(x),y)
        end
        Flux.update!(state,Î±,âˆ‡[1])
    end
end

Î´ =encoderlayers(d*2,1,d*2-2)|>gpu

@showprogress for _ in 1:Î´.epochs
     map(Î´.loader) do (x,y)
        E = Î±.encoder(x)

        state_Î´ = Flux.setup(Î´.opt,Î´)

        function loss(m)
            Î´.loss(Î±.decoder(m(E)(E')'),y)
        end
         
        âˆ‡ = Flux.gradient(loss,Î´)
        Flux.update!(state_Î´,Î´,âˆ‡[1]);
    end
end

Ïˆ = distenc(X,Î±,2*d-2;Ïƒ=Ïƒ,Î»=0.01)

